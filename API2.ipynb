{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "156e2c98-0ac2-4114-91d9-c6756f6756d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50']\n",
      "cpu\n",
      "Loading vision model config from C:\\Users\\Aurora\\AppData\\Roaming\\Python\\Python310\\site-packages\\cn_clip\\clip\\model_configs\\ViT-H-14.json\n",
      "Loading text model config from C:\\Users\\Aurora\\AppData\\Roaming\\Python\\Python310\\site-packages\\cn_clip\\clip\\model_configs\\RoBERTa-wwm-ext-large-chinese.json\n",
      "Model info {'embed_dim': 1024, 'image_resolution': 224, 'vision_layers': 32, 'vision_width': 1280, 'vision_head_width': 80, 'vision_patch_size': 14, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 1024, 'text_initializer_range': 0.02, 'text_intermediate_size': 4096, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 16, 'text_num_hidden_layers': 24, 'text_type_vocab_size': 2}\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import cn_clip\n",
    "import cn_clip.clip as clip\n",
    "from cn_clip.clip import load_from_name, available_models, load\n",
    "\n",
    "print(\"Available models:\", available_models())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "# model, preprocess = load_from_name(\"ViT-B-16\", device=device, download_root='./pt_models')\n",
    "model, preprocess = load_from_name(\"ViT-H-14\", device=device, download_root='../02_CLIP/pt_models')  # \n",
    "model.eval()\n",
    "print('finished')\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib import request\n",
    "def url2pil(img_url):\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'\n",
    "    headers = {'user-agent': user_agent}\n",
    "    req = request.Request(url=img_url, headers=headers)\n",
    "    response = request.urlopen(req, timeout=30)\n",
    "    img= Image.open(BytesIO(response.read())).convert('RGB')\n",
    "    return img\n",
    "\n",
    "def get_img_emb(url):\n",
    "    if not url: return ('1'+'***'+'url')\n",
    "    try:\n",
    "        img = url2pil(url)\n",
    "    except:\n",
    "        return ('2'+'***'+url)\n",
    "    image = url2pil(url)\n",
    "    with torch.no_grad():\n",
    "        image = preprocess(image).unsqueeze(0).to(device)\n",
    "        image_features = model.encode_image(image)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        image_features = image_features.cpu().numpy()\n",
    "        emb = ','.join([str(x) for x in image_features.flatten()])\n",
    "        return \"***\".join(('0',url, emb))\n",
    "        \n",
    "#get_img_emb(url)\n",
    "\n",
    "\n",
    "def get_txt_emb_batch(text):\n",
    "    text = clip.tokenize(text).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text)\n",
    "    text_features = text_features.detach()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features.cpu().numpy()\n",
    "    return text_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9303aeb-1472-4961-9979-e98f65b62f97",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import cn_clip.clip as clip\n",
    "from cn_clip.clip import load_from_name\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load CLIP model and preprocessing\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = load_from_name(\"ViT-H-14\", device=device, download_root='../02_CLIP/pt_models')\n",
    "model.eval()\n",
    "\n",
    "# Load text tokenizer\n",
    "text_tokenizer = clip.tokenize\n",
    "\n",
    "# Define a simple classifier model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Calculate the input size for the classifier\n",
    "text_feature_dim = model.text_projection.shape[1]  # Change this based on the correct attribute for text features\n",
    "image_feature_dim = model.visual.output_dim  # Change this based on the correct attribute for image features\n",
    "input_size = text_feature_dim + image_feature_dim\n",
    "num_classes = 2\n",
    "\n",
    "# Train and save the classifier\n",
    "classifier = Classifier(input_size, num_classes)\n",
    "classifier.to(device)\n",
    "\n",
    "# Assuming you have your own training data and training loop here...\n",
    "\n",
    "# Save the classifier weights\n",
    "torch.save(classifier.state_dict(), \"your_classifier.pth\")\n",
    "\n",
    "# Load the saved classifier\n",
    "classifier = Classifier(input_size, num_classes)\n",
    "classifier.to(device)\n",
    "classifier.load_state_dict(torch.load(\"your_classifier.pth\"))\n",
    "classifier.eval()\n",
    "\n",
    "# Get input from user\n",
    "user_text = input(\"Enter a piece of text: \")\n",
    "image_path = input(\"Enter the path to an image: \")\n",
    "\n",
    "# Process text\n",
    "text = clip.tokenize([user_text]).to(device)\n",
    "\n",
    "# Process image\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "image = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Calculate features\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text)\n",
    "    image_features = model.encode_image(image)\n",
    "\n",
    "# Concatenate text and image features\n",
    "concatenated_features = torch.cat((text_features, image_features), dim=1)\n",
    "\n",
    "# Pass features through classifier\n",
    "with torch.no_grad():\n",
    "    logits = classifier(concatenated_features)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    match_probability = probs[0, 1].item()  # Probability of match\n",
    "\n",
    "print(\"Match probability:\", match_probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e3a0b-500b-426b-a001-a057becf5e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864d9de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
